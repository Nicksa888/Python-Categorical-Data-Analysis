{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Changing column names using mapping\n",
    "    my_list = [\"iyear\", \"imonth\", \"iday\", \"extended\", \"country_txt\", \"region_txt\", \"provstate\", \"city\", \"success\", \"multiple\", \"suicide\", \"attacktype1_txt\", \"gname\",\n",
    "               \"targtype1_txt\", \"natlty1_txt\", \"weaptype1_txt\", \"nkill\"]\n",
    "    new_list = [\"Year\", \"Month\", \"Day\", \"Extended\", \"Country\", \"Region\", \"Province\", \"City\", \"Success\", \"Multiple\", \"Suicide\", \"Attack\", \"Group\", \"Target\", \"Nationality\",\n",
    "                \"Weapon\", \"Dead\"]\n",
    "\n",
    "    column_mapping = dict(zip(my_list, new_list))\n",
    "    df = df.rename(columns=column_mapping)\n",
    "\n",
    "    # Converting NA values in Dead column to zeros\n",
    "    df['Dead'] = df['Dead'].fillna(0)\n",
    "\n",
    "    # Selecting only the specified columns\n",
    "    selected_columns = ['Year', 'Month', 'Day', 'Country', 'Region', 'Province', 'City',  \n",
    "                        'Attack', 'Target', 'Group', 'Weapon', 'Dead']\n",
    "\n",
    "    df = df[selected_columns]\n",
    "\n",
    "    # Creating Lethal variable as binary\n",
    "    df['Lethal'] = np.where(df['Dead'] == 0, 0, 1)\n",
    "\n",
    "    # Mapping values in the Attack column\n",
    "    attack_mapping = {\n",
    "        \"Bombing/Explosion\": \"BombAttack\",\n",
    "        \"Hostage Taking (Kidnapping)\": \"HostageKidnapAttack\",\n",
    "        \"Facility/Infrastructure Attack\": \"InfrastructureAttack\",\n",
    "        \"Armed Assault\": \"ArmedAssaultAttack\",\n",
    "        \"Unarmed Assault\": \"UnarmedAssaultAttack\",\n",
    "        \"Hostage Taking (Barricade Incident)\": \"HostageBarricadeAttack\"\n",
    "    }\n",
    "\n",
    "    df['Attack'] = df['Attack'].map(attack_mapping).fillna(df['Attack'])\n",
    "\n",
    "    # Mapping values in the Target column\n",
    "    target_mapping = {\n",
    "        \"Private Citizens & Property\": \"Private\",\n",
    "        \"Government (Diplomatic)\": \"GovtDip\",\n",
    "        \"Journalists & Media\": \"JournalistsMedia\",\n",
    "        \"Government (General)\": \"GovtGen\",\n",
    "        \"Airports & Aircraft\": \"AirportsAircraft\",\n",
    "        \"Educational Institution\": \"EduIns\",\n",
    "        \"Violent Political Party\": \"VPPTarget\",\n",
    "        \"Religious Figures/Institutions\": \"RelFigIns\",\n",
    "        \"Unknown\": \"UnknownTarget\",\n",
    "        \"Food or Water Supply\": \"FoodWaterSup\",\n",
    "        \"Terrorists/Non-State Militia\": \"TNSMTarget\",\n",
    "        \"Abortion Related\": \"Abortion\"\n",
    "    }\n",
    "\n",
    "    df['Target'] = df['Target'].map(target_mapping).fillna(df['Target'])\n",
    "\n",
    "    # Mapping values in the Group column\n",
    "    group_mapping = {\n",
    "        \"Group.Islamic State of Iraq and the Levant (ISIL)\": \"ISIS\",\n",
    "        \"Tehrik-i-Taliban Pakistan (TTP)\": \"TTP\",\n",
    "        \"Revolutionary Armed Forces of Colombia (FARC)\": \"FARC\",\n",
    "        \"M-19 (Movement of April 19)\": \"M19\",\n",
    "        \"National Liberation Army of Colombia (ELN)\": \"ELN\",\n",
    "        \"Unknown\": \"OtherGroup\",\n",
    "        \"Tupac Amaru Revolutionary Movement (MRTA)\": \"MRTA\",\n",
    "        \"Shining Path (SL)\": \"ShiningPath\",\n",
    "        \"Salafist Group for Preaching and Fighting (GSPC)\": \"GSPC\",\n",
    "        \"Islamic Salvation Front (FIS)\": \"FIS\",\n",
    "        \"Algerian Islamic Extremists\": \"Algerian_Islamic_Extremists\",\n",
    "        \"Al-Qaida in the Islamic Maghreb (AQIM)\": \"AQIM\",\n",
    "        \"Armed Islamic Group (GIA)\": \"GIA\",\n",
    "        \"Farabundo Marti National Liberation Front (FMLN)\": \"FMLN\",\n",
    "        \"Liberation Tigers of Tamil Eelam (LTTE)\": \"LTTE\"\n",
    "    }\n",
    "\n",
    "    df['Group'] = df['Group'].map(group_mapping).fillna(df['Group'])\n",
    "\n",
    "    # Mapping values in the Province column\n",
    "    province_mapping = {\n",
    "        \"North-West Frontier Province\": \"NWFP\",\n",
    "        \"Federally Administered Tribal Areas\": \"FATA\",\n",
    "        \"Khyber Pakhtunkhwa\": \"Khyber_Pakhtunkhwa\",\n",
    "        \"Al Anbar\": \"Al_Anbar\",\n",
    "        \"Tizi Ouzou\": \"Tizi_Ouzou\",\n",
    "        \"North Central\": \"NorthCentral\",\n",
    "        \"Valle del Cauca\": \"ValledelCauca\",\n",
    "        \"Bogota\": \"BogotaProvince\"\n",
    "    }\n",
    "\n",
    "    df['Province'] = df['Province'].replace(province_mapping).fillna(df['Province'])\n",
    "\n",
    "    # Mapping values in the Weapon column\n",
    "    weapon_mapping = {\n",
    "        \"Unknown\": \"OtherWeapon\"\n",
    "    }\n",
    "\n",
    "    df['Weapon'] = df['Weapon'].replace(weapon_mapping).fillna(df['Weapon'])\n",
    "\n",
    "    # Grouping infrequent categories with a 5% threshold\n",
    "    categorical_columns = ['Attack', 'Target', 'Group', 'Province', 'Weapon', 'Country', 'City']\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        counts = df[column].value_counts(normalize=True)\n",
    "        infrequent_categories = counts[counts < 0.05].index\n",
    "        df[column] = np.where(df[column].isin(infrequent_categories), 'Other' + column, df[column])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def preprocess_dataframe_three(df, target_column, correlation_threshold=0.7, variance_threshold=0.1):\n",
    "    # Remove NA values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Check for linearity\n",
    "    linear_features = []\n",
    "    for column in df.columns:\n",
    "        if column == 'Lethal' or column == 'Year':  # Skip 'Lethal' and 'Year' variables\n",
    "            continue\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            try:\n",
    "                correlation, _ = pearsonr(df[column], df[target_column])\n",
    "            except RuntimeWarning:\n",
    "                continue\n",
    "        if np.isnan(correlation):\n",
    "            continue\n",
    "        if abs(correlation) >= correlation_threshold:\n",
    "            linear_features.append(column)\n",
    "\n",
    "    # Check for correlation\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = df.corr()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "                try:\n",
    "                    correlation = correlation_matrix.iloc[i, j]\n",
    "                except RuntimeWarning:\n",
    "                    continue\n",
    "            if np.isnan(correlation):\n",
    "                continue\n",
    "            if abs(correlation) >= correlation_threshold:\n",
    "                colname_i = correlation_matrix.columns[i]\n",
    "                colname_j = correlation_matrix.columns[j]\n",
    "                correlated_features.add(colname_i)\n",
    "                correlated_features.add(colname_j)\n",
    "\n",
    "    # Check for zero and near-zero variance\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    selector.fit(df)\n",
    "    low_variance_features = df.columns[~selector.get_support()]\n",
    "\n",
    "    # Combine all features to be removed\n",
    "    features_to_remove = set(linear_features + list(correlated_features) + list(low_variance_features))\n",
    "\n",
    "    # Return processed DataFrame with all features removed\n",
    "    return df.drop(features_to_remove, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def run_boruta_iterations(data, target, n_iterations=10, random_seed=42, file_prefix='boruta'):\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # overcome an issue of numpy compatibility with Boruta, where np.int, np.float, and np.bool should be int, float, and bool respectively\n",
    "    np.int = int\n",
    "    np.float = float\n",
    "    np.bool = bool\n",
    "\n",
    "    # Store the results of each iteration\n",
    "    results = []\n",
    "    selected_variables = []\n",
    "\n",
    "    # Create a set to store common character strings\n",
    "    common_strings = set()\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Create a new Boruta instance for each iteration\n",
    "        estimator = RandomForestClassifier(n_estimators=100)  # Choose your desired estimator\n",
    "        boruta = BorutaPy(estimator=estimator)\n",
    "        boruta.fit(data.values, target.values)\n",
    "\n",
    "        # Store the results of the current iteration\n",
    "        feature_ranks = pd.DataFrame({\n",
    "            'Feature': data.columns,\n",
    "            'Rank': boruta.ranking_,\n",
    "            'Support': boruta.support_\n",
    "        })\n",
    "        results.append(feature_ranks)\n",
    "\n",
    "        # Store the selected variables of the current iteration\n",
    "        selected_vars = feature_ranks.loc[boruta.support_, 'Feature'].tolist()\n",
    "        selected_variables.append(selected_vars)\n",
    "\n",
    "        # Add selected variables to the common_strings set\n",
    "        if i == 0:\n",
    "            common_strings.update(selected_vars)\n",
    "        else:\n",
    "            common_strings.intersection_update(selected_vars)\n",
    "\n",
    "        # Save the results of the current iteration to a file\n",
    "        file_name = f'{file_prefix}_iteration_{i+1}.csv'\n",
    "        feature_ranks.to_csv(file_name, index=False)\n",
    "\n",
    "        # Print the results of the current iteration\n",
    "        print(f\"Iteration {i+1} results:\")\n",
    "        print(feature_ranks)\n",
    "\n",
    "        # Print the selected variables of the current iteration\n",
    "        print(f\"Selected variables in iteration {i+1}:\")\n",
    "        print(selected_vars)\n",
    "\n",
    "        # Print the common strings after each iteration\n",
    "        print(f\"Common strings after iteration {i+1}:\")\n",
    "        print(common_strings)\n",
    "        print()\n",
    "\n",
    "    return results, selected_variables, common_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "def generate_lasso_iterations_one(X, y):\n",
    "    np.random.seed(42)  # Set a seed for reproducibility\n",
    "    vectors = []  # Initialize an empty list to store the vectors\n",
    "    \n",
    "    for i in range(10):\n",
    "        random_seed = np.random.randint(1000)  # Generate a random seed for each iteration\n",
    "        model = LassoCV(random_state = random_seed, cv = 10)\n",
    "        \n",
    "        # Fit the model and perform cross-validation\n",
    "        model.fit(X, y)\n",
    "        scores = cross_val_score(model, X, y, cv = 10)\n",
    "        \n",
    "        # Print the average cross-validation score\n",
    "        print(f\"Iteration {i + 1} - Average Cross-Validation Score: {np.mean(scores):.3f}\")\n",
    "        \n",
    "        # Print the optimal collection of coefficients\n",
    "        print(f\"Iteration {i + 1} - Optimal Coefficients:\")\n",
    "        vector = []  # Initialize an empty list for the current iteration's vector\n",
    "        for feature, coef in zip(X.columns, model.coef_):\n",
    "            if coef != 0:  # Only consider coefficients for non-zero features\n",
    "                print(f\"{feature}: {coef}\")\n",
    "                vector.append(feature)  # Append the feature to the vector list\n",
    "        \n",
    "        vectors.append(vector)  # Append the vector to the vectors list\n",
    "        print()\n",
    "    \n",
    "    # Find common features in all the vectors\n",
    "    common_features = set.intersection(*map(set, vectors))\n",
    "    print(\"Common Features in All Vectors:\")\n",
    "    for feature in common_features:\n",
    "        print(feature)\n",
    "    \n",
    "    return vectors, common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc870227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "def caret_model_proportion(data, target_col, Output, Output_Two, Output_Three, test_size=0.2):\n",
    "    # Set the seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Create dictionaries to store the model outputs and performance metrics\n",
    "    model_outputs = {}\n",
    "    performance_metrics = {}\n",
    "    \n",
    "    # Define the sampling methods and models\n",
    "    sampling_methods = [\"none\", \"boot\", \"LGOCV\", \"cv\", \"repeatedcv\"]\n",
    "    models = [LogisticRegression(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
    "    \n",
    "    # Split the data into train and test sets using stratified random sampling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(target_col, axis = 1),\n",
    "        data[target_col],\n",
    "        test_size = test_size,\n",
    "        stratify = data[target_col],\n",
    "        random_state = 123\n",
    "    )\n",
    "    \n",
    "    # Iterate through sampling methods and models\n",
    "    for sampling_method in sampling_methods:\n",
    "        for model in models:\n",
    "            # Set the seed for each model\n",
    "            np.random.seed(123)\n",
    "            \n",
    "            # Train the caret model\n",
    "            caret_formula = target_col + \" ~ .\"\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Save the model output\n",
    "            model_outputs[sampling_method + \"_\" + model.__class__.__name__] = model\n",
    "            \n",
    "            # Save the model object\n",
    "            output_file = Output + \"_\" + sampling_method + \"_\" + model.__class__.__name__ + \"Output.pkl\"\n",
    "            joblib.dump(model, output_file)\n",
    "            \n",
    "            # Make predictions on the test data\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate the confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Convert confusion matrix values to percentages\n",
    "            row_sums = cm.sum(axis=1, keepdims=True)\n",
    "            cm_percent = (cm / row_sums) * 100\n",
    "            cm_percent_rounded = np.round(cm_percent, decimals=2)\n",
    "            cm_percent_formatted = [\"{:.2f}%\".format(value) for value in cm_percent_rounded.ravel()]\n",
    "            cm_percent_reshaped = np.array(cm_percent_formatted).reshape(cm_percent_rounded.shape)\n",
    "            \n",
    "            # Calculate performance metrics and convert to percentages\n",
    "            accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "            precision = precision_score(y_test, y_pred) * 100\n",
    "            recall = recall_score(y_test, y_pred) * 100\n",
    "            f1 = f1_score(y_test, y_pred) * 100\n",
    "            \n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            specificity = (tn / (tn + fp)) * 100\n",
    "            \n",
    "            # Save the performance metrics\n",
    "            performance_metrics[sampling_method + \"_\" + model.__class__.__name__] = {\n",
    "                'Confusion Matrix': cm_percent_reshaped,\n",
    "                'Accuracy': \"{:.2f}%\".format(accuracy),\n",
    "                'Precision': \"{:.2f}%\".format(precision),\n",
    "                'Recall': \"{:.2f}%\".format(recall),\n",
    "                'F1-Score': \"{:.2f}%\".format(f1),\n",
    "                'Specificity': \"{:.2f}%\".format(specificity)\n",
    "            }\n",
    "    \n",
    "    return model_outputs, performance_metrics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
